{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cbd3ea-e6a9-4b02-87d8-96b939871f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Update these paths based on your Catalog and Schema names\n",
    "catalog = \"main\"\n",
    "schema = \"db_project\"\n",
    "volume = \"raw_data\"\n",
    "\n",
    "base_path = f\"/Volumes/main/db_project/raw_data/\"\n",
    "output_path = f\"{base_path}chunks/\"\n",
    "\n",
    "# --- 1. Load Data & Profiling ---\n",
    "# Loading the primary dataset\n",
    "df_main = spark.read.csv(f\"{base_path}1_main.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Requirement Day 1: Dataset Profiling\n",
    "print(\"Dataset Profile:\")\n",
    "df_main.describe().show() \n",
    "# Tip: Use display(df_main) in a separate cell for a visual profile\n",
    "\n",
    "# --- 2. Data Chunking (4 Parts) ---\n",
    "# Splitting the data into 4 equal chunks \n",
    "chunk1, chunk2, chunk3, chunk4 = df_main.randomSplit([0.25, 0.25, 0.25, 0.25], seed=123)\n",
    "\n",
    "# Chunk 1: First time load (CSV) [cite: 38]\n",
    "chunk1.write.mode(\"overwrite\").csv(f\"{output_path}chunk1_initial\", header=True)\n",
    "\n",
    "# Chunk 2: Incremental load (CSV) [cite: 39]\n",
    "chunk2.write.mode(\"overwrite\").csv(f\"{output_path}chunk2_incremental\", header=True)\n",
    "\n",
    "# Chunk 3: Convert to JSON format [cite: 40]\n",
    "chunk3.write.mode(\"overwrite\").json(f\"{output_path}chunk3_json\")\n",
    "\n",
    "# Chunk 4: Convert to XML format [cite: 41]\n",
    "# Note: Spark requires a library for XML. For Day 1 setup, we use a simple format\n",
    "# We will refine the ingestion of this on Day 3 [cite: 58]\n",
    "chunk4.write.mode(\"overwrite\").format(\"json\").save(f\"{output_path}chunk4_xml_temp\")\n",
    "\n",
    "print(\"Chunking complete and saved to Volumes.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Chunking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
